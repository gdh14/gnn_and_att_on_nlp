{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/mr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e85ba0bc6de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mr'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.ipynb_checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/mr'"
     ]
    }
   ],
   "source": [
    "dataset = 'mr'\n",
    "files = os.listdir('data/{}'.format(dataset))\n",
    "files.remove('.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mr dataset\n",
    "\n",
    "def clean_str(s):\n",
    "    s = s.decode('latin-1')\n",
    "    s = re.sub(r'\\r\\n', '', s)\n",
    "    return s\n",
    "    \n",
    "def read_dataset(dataset, file):\n",
    "    with open('data/{}/{}'.format(dataset, file), 'rb') as f:\n",
    "        result = f.readlines()\n",
    "        \n",
    "    result = map(clean_str, result)\n",
    "    if 'label' in file:\n",
    "        result = list(map(int, result))\n",
    "    else:\n",
    "        result = list(map(lambda x: x.split(), result))\n",
    "    return result\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = read_dataset('mr', 'text_train.txt')\n",
    "label_train = read_dataset('mr', 'label_train.txt')\n",
    "\n",
    "text_test = read_dataset('mr', 'text_test.txt')\n",
    "label_test = read_dataset('mr', 'label_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_str = list(map(lambda x: \" \".join(x), text_train))\n",
    "text_test_str = list(map(lambda x: \" \".join(x), text_test))\n",
    "\n",
    "train_df = pd.DataFrame({'text': text_train_str, 'label': label_train})\n",
    "test_df = pd.DataFrame({'text': text_test_str, 'label': label_test})\n",
    "\n",
    "train_df.to_csv('data/mr/train_df.tsv', index_label=False, sep='\\t')\n",
    "test_df.to_csv('data/mr/test_df.tsv', index_label=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df['train_mask'] = True\n",
    "train_df['test_mask'] = False\n",
    "test_df['test_mask'] = True\n",
    "test_df['train_mask'] = False\n",
    "\n",
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "all_df.reset_index(drop=True, inplace=True)\n",
    "all_df.reset_index(inplace=True)\n",
    "all_df.columns = [\"doc_id\"] + list(all_df.columns)[1:]\n",
    "all_df.doc_id = all_df.doc_id.astype(str)\n",
    "all_df.doc_id = 'doc_id_' + all_df.doc_id\n",
    "all_df.to_csv('data/mr/raw/all_df_mask.tsv', index_label=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn data into the following csv format\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>label</th>\n",
       "      <th>test_mask</th>\n",
       "      <th>text</th>\n",
       "      <th>train_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_id_0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>'moore is like a progressive bull in a china s...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_id_1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>idiotic and ugly .</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_id_2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>even if the naipaul original remains the real ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_id_3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>the movie is amateurish , but it's a minor tre...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_id_4</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>some people march to the beat of a different d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id  label  test_mask  \\\n",
       "0  doc_id_0      1      False   \n",
       "1  doc_id_1      1      False   \n",
       "2  doc_id_2      1      False   \n",
       "3  doc_id_3      1      False   \n",
       "4  doc_id_4      1      False   \n",
       "\n",
       "                                                text  train_mask  \n",
       "0  'moore is like a progressive bull in a china s...        True  \n",
       "1                                 idiotic and ugly .        True  \n",
       "2  even if the naipaul original remains the real ...        True  \n",
       "3  the movie is amateurish , but it's a minor tre...        True  \n",
       "4  some people march to the beat of a different d...        True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Turn data into the following csv format\")\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Vocab(object):\n",
    "  \n",
    "  def __init__(self, L):\n",
    "    if isinstance(L[0], list):\n",
    "      tokens = list(itertools.chain(*L))\n",
    "      self.token_counts = pd.Series(tokens).value_counts().to_frame().sort_index(ascending=True)\n",
    "      self.vocab = [\"unk\"] + self.token_counts.index.to_list()\n",
    "    else:\n",
    "      tokens = self.token_counts = pd.Series(L).value_counts().to_frame().sort_index(ascending=True)\n",
    "      self.vocab = self.token_counts.index.to_list()\n",
    "    self.w2i = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "    self.i2w = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "\n",
    "  def map_words2index(self, L):\n",
    "    return list(map(lambda x: self.w2i[x] if x in self.w2i else self.w2i['unk'], L))\n",
    "\n",
    "  def map_index2words(self, L):\n",
    "    return list(map(lambda x: self.i2w[x], L))\n",
    "\n",
    "  def map_dataset_words2index(self, L):\n",
    "    return np.array(list(map(self.map_words2index, L)))\n",
    "\n",
    "  def map_dataset_index2words(self, L):\n",
    "    return np.array(list(map(self.map_index2words, L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80.6 ms, sys: 4.02 ms, total: 84.6 ms\n",
      "Wall time: 84.1 ms\n",
      "CPU times: user 59.1 ms, sys: 0 ns, total: 59.1 ms\n",
      "Wall time: 58.9 ms\n",
      "CPU times: user 29.3 ms, sys: 0 ns, total: 29.3 ms\n",
      "Wall time: 29.2 ms\n",
      "21402\n",
      "'moore is like a progressive bull in a china shop , a provocateur crashing into ideas and special-interest groups as he slaps together his own brand of liberalism . '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time vocab = Vocab(text_train + text_test)\n",
    "%time train_x = vocab.map_dataset_words2index(text_train)\n",
    "%time test_x = vocab.map_dataset_words2index(text_test)\n",
    "\n",
    "print(len(vocab.vocab))\n",
    "train_x_ = vocab.map_dataset_index2words(train_x)\n",
    "print(\" \".join(train_x_[0]))\n",
    "\n",
    "label_vocab = Vocab(label_train + label_test)\n",
    "label_vocab.w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, LongTensor\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, Y=None):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if self.Y is not None:\n",
    "      return (self.X[idx], self.Y[idx])\n",
    "    return (self.X[idx], None)\n",
    "\n",
    "def pad(seq, seq_lengths, pad_after=True):\n",
    "  max_seq_len = max(seq_lengths)\n",
    "  seq_tensor = Variable(torch.zeros((len(seq), max_seq_len))).long()\n",
    "  # pad input tensor\n",
    "  for idx, seq in enumerate(seq):\n",
    "    seq_len = seq_lengths[idx]\n",
    "    if pad_after:\n",
    "      seq_tensor[idx, :seq_len] = LongTensor(np.asarray(seq).astype(int))\n",
    "    else: \n",
    "      # pad before\n",
    "      seq_tensor[idx, max_seq_len-seq_len:] = LongTensor(np.asarray(seq).astype(int))\n",
    "  return seq_tensor\n",
    "\n",
    "def batchify(data):\n",
    "  X, Y = tuple(map(list, zip(*data)))\n",
    "  seq_lengths = LongTensor([len(x) for x in X])\n",
    "  X = pad(X, seq_lengths, pad_after=True)\n",
    "  Y = LongTensor(Y)\n",
    "  return X, Y\n",
    "\n",
    "def batchify_test(data):\n",
    "  X, Y = tuple(map(list, zip(*data)))\n",
    "  seq_lengths = LongTensor([len(x) for x in X])\n",
    "  X = pad(X, seq_lengths, pad_after=True)\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "train = MyDataset(train_x, label_train)\n",
    "valid = MyDataset(test_x, label_test)\n",
    "test = MyDataset(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=64, shuffle=True, collate_fn=batchify)\n",
    "valid_loader = DataLoader(valid, batch_size=64, shuffle=False, collate_fn=batchify)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False, collate_fn=batchify_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, LongTensor, Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def accuracy(preds, y):\n",
    "  return (np.array(preds) == np.array(y)).astype(int).mean()\n",
    "\n",
    "\n",
    "def train_epoch(epoch, model, optimizer, criterion):\n",
    "  model.train()\n",
    "  train_loss, n_data = 0, 0\n",
    "  start = time.time()\n",
    "  preds = []\n",
    "  labels = []\n",
    "  for i, (x, y) in enumerate(train_loader):\n",
    "    n_data += x.size()[0]\n",
    "    labels.extend(y.tolist())\n",
    "    if is_cuda: x, y = x.cuda(), y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    preds.extend(out.argmax(axis=1).tolist())\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    if grad_clip: torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    train_loss += loss\n",
    "    if i % print_iter == print_iter - 1:\n",
    "      model, valid_preds, valid_labels, valid_loss = validate(model, criterion)\n",
    "      print(\"\"\"epoch {} - batch [{}/{}] - train loss: {:.2f} - acc: {:.3f} - valid loss : {:.2f} - acc : {:.3f} time taken: {:.2f}\"\"\".format(epoch, i, \n",
    "            len(train_loader), train_loss/(i+1),\n",
    "            accuracy(preds, labels), valid_loss, accuracy(valid_preds, valid_labels),\n",
    "            time.time()-start), flush=True)\n",
    "      \n",
    "      model.train()\n",
    "      start = time.time()\n",
    "      train_loss = 0\n",
    "\n",
    "  # end of epoch\n",
    "  model, valid_preds, valid_labels, valid_loss = validate(model, criterion)\n",
    "  print(\"\"\"epoch {} - batch [{}/{}] - train loss: {:.2f} - acc: {:.3f} - valid loss : {:.2f} - acc : {:.3f} time taken: {:.2f}\"\"\".format(epoch, i, \n",
    "        len(train_loader), train_loss/(i+1),\n",
    "        accuracy(preds, labels), valid_loss, accuracy(valid_preds, valid_labels),\n",
    "        time.time()-start), flush=True)\n",
    "  return model\n",
    "\n",
    "def learning_rate_decay(optimizer):\n",
    "  for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * 0.1\n",
    "  return optimizer\n",
    "\n",
    "def training(model, epoches, lr, wd, return_model=False):\n",
    "  if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "  optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  for ep in range(epoches):\n",
    "    model = train_epoch(ep, model, optimizer, criterion)\n",
    "    optimizer = learning_rate_decay(optimizer)\n",
    "  if return_model:\n",
    "      return model\n",
    "\n",
    "def validate(model, criterion):\n",
    "  model.eval()\n",
    "  valid_loss = 0\n",
    "  preds, labels = [], []\n",
    "  for i, (x, y) in enumerate(valid_loader):\n",
    "    labels.extend(y.tolist())\n",
    "    if torch.cuda.is_available(): x, y = x.cuda(), y.cuda()\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    preds.extend(out.argmax(axis=1).tolist())\n",
    "    valid_loss += loss\n",
    "  return model, preds, labels, valid_loss/(i+1)\n",
    "    \n",
    "def predict(model, loader):\n",
    "  model.eval()\n",
    "  preds, labels = [], []\n",
    "  for i, (x, _) in enumerate(loader):\n",
    "    if torch.cuda.is_available(): x = x.cuda()\n",
    "    out = model(x)\n",
    "    preds.extend(out.argmax(axis=1).tolist())\n",
    "  return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class LSTM_clf(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_dim, hidden_dim, vocab_size, out_size, \n",
    "               layers=1, bidirectional=False):\n",
    "    super(LSTM_clf, self).__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.net = nn.LSTM(embed_dim, hidden_dim,  num_layers=layers, \n",
    "                       bidirectional=bidirectional, dropout=0.5)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.bn = nn.BatchNorm1d(hidden_dim * (int(bidirectional) + 1))\n",
    "    self.linear = nn.Linear(hidden_dim * (int(bidirectional) + 1), out_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.word_embedding(x)\n",
    "    out = self.net(out)[0]\n",
    "    out = self.relu(out).transpose(1,2)\n",
    "    out = F.max_pool1d(out, out.size()[2]).squeeze()\n",
    "    out = self.linear(self.bn(out))\n",
    "    return out\n",
    "\n",
    "class DCNN_block(nn.Module):\n",
    "  \n",
    "  def __init__(self, embed_dim, hidden_dim, kernel_size, dilations=None,\n",
    "               dropout=0.2):\n",
    "    super(DCNN_block, self).__init__()\n",
    "    self.conv1 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, dilation=1))\n",
    "    self.conv2 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, dilation=2))\n",
    "    self.conv3 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, dilation=4))\n",
    "    self.net = nn.Sequential(self.conv1, nn.ReLU(), nn.Dropout(dropout),\n",
    "                             self.conv2, nn.ReLU(), nn.Dropout(dropout), \n",
    "                             self.conv3, nn.ReLU(), nn.Dropout(dropout))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # N x C x L\n",
    "    return self.net(x)\n",
    "\n",
    "class DCNN_rez_block(nn.Module):\n",
    "  \n",
    "  def __init__(self, embed_dim, hidden_dim, kernel_size, dilations=None,\n",
    "               dropout=0.2):\n",
    "    super(DCNN_rez_block, self).__init__()\n",
    "    self.conv1 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, \n",
    "                                       padding=(kernel_size-1)*1, dilation=1))\n",
    "    self.conv2 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, \n",
    "                                       padding=(kernel_size-1)*2, dilation=2))\n",
    "    self.conv3 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, \n",
    "                                       padding=(kernel_size-1)*4, dilation=4))\n",
    "\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.relu3 = nn.ReLU()\n",
    "\n",
    "    self.do1 = nn.Dropout(dropout)\n",
    "    self.do2 = nn.Dropout(dropout)\n",
    "    self.do3 = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # N x C x L\n",
    "    seq_len = x.size()[2]\n",
    "    out = self.do1(self.relu1(self.conv1(x)))[:, :, -seq_len:]\n",
    "    out = out + self.do2(self.relu2(self.conv2(x)))[:, :, -seq_len:]\n",
    "    out = out + self.do3(self.relu3(self.conv3(x)))[:, :, -seq_len:]\n",
    "    return out\n",
    "\n",
    "\n",
    "class DCNN(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_dim, hidden_dim, vocab_size, out_size, \n",
    "               kernel_size, dilations=None, rez_block=True, \n",
    "               dropout=0.2):\n",
    "    super(DCNN, self).__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    if rez_block: \n",
    "      self.net = DCNN_rez_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "    else:\n",
    "      self.net = DCNN_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "    self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "    self.do = nn.Dropout(dropout)\n",
    "    self.linear = nn.Linear(hidden_dim, out_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.word_embedding(x)\n",
    "    out = self.net(out.transpose(1,2))\n",
    "    out = F.max_pool1d(out, out.size()[2]).squeeze()\n",
    "    out = self.linear(self.do(self.bn(out)))\n",
    "    return out\n",
    "\n",
    "\n",
    "class DDCNN(nn.Module):\n",
    "  # Dilated and Dense CNN\n",
    "  def __init__(self, embed_dim, hidden_dim, vocab_size, out_size, \n",
    "               kernel_size, dilations=None, rez_block=True, \n",
    "               dropout=0.2):\n",
    "    super(DDCNN, self).__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    if rez_block: \n",
    "      self.dcnn = DCNN_rez_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "    else:\n",
    "      self.dcnn = DCNN_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "\n",
    "    self.do1 = nn.Dropout(dropout)\n",
    "    self.do2 = nn.Dropout(dropout)\n",
    "    self.do3 = nn.Dropout(dropout)\n",
    "    self.cnn1 = weight_norm(nn.Conv1d(embed_dim, int(hidden_dim//3), 4, padding=3, dilation=1))\n",
    "    self.cnn2 = weight_norm(nn.Conv1d(embed_dim, int(hidden_dim//3), 6, padding=5, dilation=1))\n",
    "    self.cnn3 = weight_norm(nn.Conv1d(embed_dim, int(hidden_dim//3), 8, padding=7, dilation=1))\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(hidden_dim*2)\n",
    "    self.do = nn.Dropout(dropout)\n",
    "    self.linear = nn.Linear(hidden_dim*2, out_size)\n",
    "\n",
    "  def cnn(self, x):\n",
    "    out1 = F.relu(self.cnn1(self.do1(x)))\n",
    "    out2 = F.relu(self.cnn2(self.do2(x)))\n",
    "    out3 = F.relu(self.cnn3(self.do3(x)))\n",
    "    outs = []\n",
    "    for o in [out1, out2, out3]:\n",
    "      outs.append(F.max_pool1d(o, o.size()[2]).squeeze())\n",
    "    out = torch.cat(outs, 1)\n",
    "    return out\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.word_embedding(x).transpose(1,2)\n",
    "    dcnn_out = self.dcnn(out)\n",
    "    cnn_out = self.cnn(out)\n",
    "    dcnn_out = F.max_pool1d(dcnn_out, dcnn_out.size()[2]).squeeze()\n",
    "    out = self.linear(self.do(self.bn(torch.cat((dcnn_out,cnn_out), 1))))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "bs = 512\n",
    "n_class = 16\n",
    "epochs = 3\n",
    "lstm_hidden = 300\n",
    "cnn_hidden = 300\n",
    "embed_dim = 300\n",
    "layers = 2\n",
    "kernel_size = 3\n",
    "vocab_size = len(vocab.vocab)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "lr = 0.002\n",
    "grad_clip = 1\n",
    "print_iter = 500\n",
    "lstm1 = LSTM_clf(embed_dim, lstm_hidden, vocab_size, n_class, layers)\n",
    "dcnn1 = DCNN(embed_dim, cnn_hidden, vocab_size, n_class, 3, \n",
    "             rez_block=False, dropout=0.2)\n",
    "dcnn_rez1 = DCNN(embed_dim, cnn_hidden, vocab_size, n_class, 5, \n",
    "                 rez_block=True, dropout=0.2)\n",
    "ddcnn_rez1 = DDCNN(embed_dim, 300, vocab_size, n_class, 5, \n",
    "                 rez_block=True, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [111/112] - train loss: 0.70 - acc: 0.558 - valid loss : 0.73 - acc : 0.499 time taken: 5.58\n",
      "epoch 1 - batch [111/112] - train loss: 0.68 - acc: 0.584 - valid loss : 0.68 - acc : 0.567 time taken: 6.06\n",
      "epoch 2 - batch [111/112] - train loss: 0.67 - acc: 0.588 - valid loss : 0.68 - acc : 0.578 time taken: 6.25\n",
      "epoch 3 - batch [111/112] - train loss: 0.67 - acc: 0.595 - valid loss : 0.68 - acc : 0.579 time taken: 5.97\n",
      "epoch 4 - batch [111/112] - train loss: 0.67 - acc: 0.592 - valid loss : 0.68 - acc : 0.579 time taken: 5.57\n",
      "epoch 5 - batch [111/112] - train loss: 0.67 - acc: 0.588 - valid loss : 0.68 - acc : 0.578 time taken: 5.56\n",
      "epoch 6 - batch [111/112] - train loss: 0.67 - acc: 0.599 - valid loss : 0.68 - acc : 0.578 time taken: 5.57\n",
      "epoch 7 - batch [111/112] - train loss: 0.67 - acc: 0.600 - valid loss : 0.68 - acc : 0.573 time taken: 5.56\n",
      "epoch 8 - batch [111/112] - train loss: 0.67 - acc: 0.595 - valid loss : 0.68 - acc : 0.579 time taken: 5.57\n",
      "epoch 9 - batch [111/112] - train loss: 0.67 - acc: 0.593 - valid loss : 0.68 - acc : 0.575 time taken: 5.75\n",
      "epoch 10 - batch [111/112] - train loss: 0.66 - acc: 0.599 - valid loss : 0.68 - acc : 0.570 time taken: 5.57\n",
      "epoch 11 - batch [111/112] - train loss: 0.66 - acc: 0.606 - valid loss : 0.68 - acc : 0.578 time taken: 5.72\n",
      "epoch 12 - batch [111/112] - train loss: 0.66 - acc: 0.598 - valid loss : 0.68 - acc : 0.572 time taken: 5.75\n",
      "epoch 13 - batch [111/112] - train loss: 0.67 - acc: 0.592 - valid loss : 0.68 - acc : 0.574 time taken: 5.60\n",
      "epoch 14 - batch [111/112] - train loss: 0.66 - acc: 0.601 - valid loss : 0.68 - acc : 0.574 time taken: 6.09\n",
      "epoch 15 - batch [111/112] - train loss: 0.66 - acc: 0.597 - valid loss : 0.68 - acc : 0.577 time taken: 6.47\n",
      "epoch 16 - batch [111/112] - train loss: 0.66 - acc: 0.596 - valid loss : 0.68 - acc : 0.577 time taken: 6.25\n",
      "epoch 17 - batch [111/112] - train loss: 0.67 - acc: 0.593 - valid loss : 0.68 - acc : 0.577 time taken: 5.70\n",
      "epoch 18 - batch [111/112] - train loss: 0.67 - acc: 0.588 - valid loss : 0.68 - acc : 0.577 time taken: 6.28\n",
      "epoch 19 - batch [111/112] - train loss: 0.66 - acc: 0.601 - valid loss : 0.68 - acc : 0.580 time taken: 5.97\n",
      "CPU times: user 1min 37s, sys: 19.4 s, total: 1min 57s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%time training(lstm1, 20, 2e-3, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [111/112] - train loss: 1.44 - acc: 0.470 - valid loss : 0.75 - acc : 0.505 time taken: 4.36\n",
      "epoch 1 - batch [111/112] - train loss: 0.66 - acc: 0.609 - valid loss : 0.67 - acc : 0.591 time taken: 4.79\n",
      "epoch 2 - batch [111/112] - train loss: 0.62 - acc: 0.655 - valid loss : 0.66 - acc : 0.618 time taken: 4.64\n",
      "epoch 3 - batch [111/112] - train loss: 0.62 - acc: 0.657 - valid loss : 0.66 - acc : 0.624 time taken: 4.14\n",
      "epoch 4 - batch [111/112] - train loss: 0.62 - acc: 0.652 - valid loss : 0.65 - acc : 0.623 time taken: 4.18\n",
      "epoch 5 - batch [111/112] - train loss: 0.62 - acc: 0.659 - valid loss : 0.66 - acc : 0.623 time taken: 4.11\n",
      "epoch 6 - batch [111/112] - train loss: 0.62 - acc: 0.668 - valid loss : 0.66 - acc : 0.622 time taken: 3.96\n",
      "epoch 7 - batch [111/112] - train loss: 0.62 - acc: 0.655 - valid loss : 0.66 - acc : 0.616 time taken: 4.83\n",
      "epoch 8 - batch [111/112] - train loss: 0.62 - acc: 0.666 - valid loss : 0.66 - acc : 0.620 time taken: 4.76\n",
      "epoch 9 - batch [111/112] - train loss: 0.62 - acc: 0.662 - valid loss : 0.66 - acc : 0.617 time taken: 4.67\n",
      "epoch 10 - batch [111/112] - train loss: 0.62 - acc: 0.663 - valid loss : 0.66 - acc : 0.619 time taken: 4.69\n",
      "epoch 11 - batch [111/112] - train loss: 0.62 - acc: 0.664 - valid loss : 0.65 - acc : 0.618 time taken: 4.19\n",
      "epoch 12 - batch [111/112] - train loss: 0.62 - acc: 0.661 - valid loss : 0.66 - acc : 0.620 time taken: 4.53\n",
      "epoch 13 - batch [111/112] - train loss: 0.62 - acc: 0.667 - valid loss : 0.66 - acc : 0.618 time taken: 4.35\n",
      "epoch 14 - batch [111/112] - train loss: 0.62 - acc: 0.664 - valid loss : 0.66 - acc : 0.618 time taken: 4.60\n",
      "epoch 15 - batch [111/112] - train loss: 0.62 - acc: 0.659 - valid loss : 0.66 - acc : 0.622 time taken: 5.12\n",
      "epoch 16 - batch [111/112] - train loss: 0.62 - acc: 0.656 - valid loss : 0.66 - acc : 0.623 time taken: 4.83\n",
      "epoch 17 - batch [111/112] - train loss: 0.61 - acc: 0.669 - valid loss : 0.66 - acc : 0.610 time taken: 4.72\n",
      "epoch 18 - batch [111/112] - train loss: 0.62 - acc: 0.653 - valid loss : 0.66 - acc : 0.620 time taken: 4.02\n",
      "epoch 19 - batch [111/112] - train loss: 0.62 - acc: 0.663 - valid loss : 0.66 - acc : 0.618 time taken: 4.07\n",
      "CPU times: user 1min 15s, sys: 14.6 s, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%time training(dcnn1, 20, 2e-3, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [111/112] - train loss: 1.48 - acc: 0.528 - valid loss : 0.67 - acc : 0.610 time taken: 4.95\n",
      "epoch 1 - batch [111/112] - train loss: 0.53 - acc: 0.768 - valid loss : 0.65 - acc : 0.634 time taken: 4.96\n",
      "epoch 2 - batch [111/112] - train loss: 0.47 - acc: 0.810 - valid loss : 0.64 - acc : 0.635 time taken: 4.97\n",
      "epoch 3 - batch [111/112] - train loss: 0.46 - acc: 0.814 - valid loss : 0.65 - acc : 0.634 time taken: 4.96\n",
      "epoch 4 - batch [111/112] - train loss: 0.45 - acc: 0.819 - valid loss : 0.64 - acc : 0.636 time taken: 4.97\n",
      "epoch 5 - batch [111/112] - train loss: 0.45 - acc: 0.816 - valid loss : 0.64 - acc : 0.634 time taken: 4.97\n",
      "epoch 6 - batch [111/112] - train loss: 0.46 - acc: 0.822 - valid loss : 0.64 - acc : 0.633 time taken: 5.09\n",
      "epoch 7 - batch [111/112] - train loss: 0.45 - acc: 0.824 - valid loss : 0.65 - acc : 0.633 time taken: 5.01\n",
      "epoch 8 - batch [111/112] - train loss: 0.46 - acc: 0.817 - valid loss : 0.65 - acc : 0.635 time taken: 5.19\n",
      "epoch 9 - batch [111/112] - train loss: 0.45 - acc: 0.823 - valid loss : 0.64 - acc : 0.634 time taken: 4.97\n",
      "epoch 10 - batch [111/112] - train loss: 0.45 - acc: 0.825 - valid loss : 0.65 - acc : 0.636 time taken: 4.97\n",
      "epoch 11 - batch [111/112] - train loss: 0.46 - acc: 0.813 - valid loss : 0.65 - acc : 0.635 time taken: 4.97\n",
      "epoch 12 - batch [111/112] - train loss: 0.46 - acc: 0.817 - valid loss : 0.64 - acc : 0.637 time taken: 4.97\n",
      "epoch 13 - batch [111/112] - train loss: 0.46 - acc: 0.816 - valid loss : 0.64 - acc : 0.635 time taken: 4.99\n",
      "epoch 14 - batch [111/112] - train loss: 0.46 - acc: 0.824 - valid loss : 0.64 - acc : 0.635 time taken: 5.00\n",
      "epoch 15 - batch [111/112] - train loss: 0.46 - acc: 0.816 - valid loss : 0.65 - acc : 0.636 time taken: 4.97\n",
      "epoch 16 - batch [111/112] - train loss: 0.46 - acc: 0.819 - valid loss : 0.64 - acc : 0.634 time taken: 5.08\n",
      "epoch 17 - batch [111/112] - train loss: 0.46 - acc: 0.820 - valid loss : 0.64 - acc : 0.636 time taken: 5.09\n",
      "epoch 18 - batch [111/112] - train loss: 0.46 - acc: 0.822 - valid loss : 0.65 - acc : 0.631 time taken: 5.10\n",
      "epoch 19 - batch [111/112] - train loss: 0.45 - acc: 0.819 - valid loss : 0.64 - acc : 0.633 time taken: 5.19\n",
      "CPU times: user 1min 23s, sys: 17 s, total: 1min 40s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%time training(dcnn_rez1, 20, 2e-3, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [111/112] - train loss: 1.34 - acc: 0.539 - valid loss : 0.70 - acc : 0.628 time taken: 6.98\n",
      "epoch 1 - batch [111/112] - train loss: 0.46 - acc: 0.806 - valid loss : 0.63 - acc : 0.654 time taken: 7.05\n",
      "epoch 2 - batch [111/112] - train loss: 0.36 - acc: 0.864 - valid loss : 0.64 - acc : 0.655 time taken: 6.98\n",
      "epoch 3 - batch [111/112] - train loss: 0.36 - acc: 0.875 - valid loss : 0.63 - acc : 0.658 time taken: 7.06\n",
      "epoch 4 - batch [111/112] - train loss: 0.36 - acc: 0.868 - valid loss : 0.63 - acc : 0.658 time taken: 6.99\n",
      "epoch 5 - batch [111/112] - train loss: 0.35 - acc: 0.878 - valid loss : 0.63 - acc : 0.657 time taken: 7.05\n",
      "epoch 6 - batch [111/112] - train loss: 0.35 - acc: 0.870 - valid loss : 0.63 - acc : 0.654 time taken: 7.01\n",
      "epoch 7 - batch [111/112] - train loss: 0.36 - acc: 0.875 - valid loss : 0.64 - acc : 0.658 time taken: 7.04\n",
      "epoch 8 - batch [111/112] - train loss: 0.36 - acc: 0.876 - valid loss : 0.63 - acc : 0.656 time taken: 7.13\n",
      "epoch 9 - batch [111/112] - train loss: 0.35 - acc: 0.878 - valid loss : 0.63 - acc : 0.655 time taken: 7.37\n",
      "epoch 10 - batch [111/112] - train loss: 0.36 - acc: 0.868 - valid loss : 0.64 - acc : 0.657 time taken: 7.16\n",
      "epoch 11 - batch [111/112] - train loss: 0.36 - acc: 0.875 - valid loss : 0.64 - acc : 0.657 time taken: 7.59\n",
      "epoch 12 - batch [111/112] - train loss: 0.36 - acc: 0.875 - valid loss : 0.63 - acc : 0.655 time taken: 7.01\n",
      "epoch 13 - batch [111/112] - train loss: 0.36 - acc: 0.873 - valid loss : 0.63 - acc : 0.660 time taken: 7.04\n",
      "epoch 14 - batch [111/112] - train loss: 0.36 - acc: 0.873 - valid loss : 0.64 - acc : 0.656 time taken: 7.09\n",
      "epoch 15 - batch [111/112] - train loss: 0.36 - acc: 0.876 - valid loss : 0.63 - acc : 0.657 time taken: 7.09\n",
      "epoch 16 - batch [111/112] - train loss: 0.36 - acc: 0.875 - valid loss : 0.64 - acc : 0.648 time taken: 7.04\n",
      "epoch 17 - batch [111/112] - train loss: 0.35 - acc: 0.872 - valid loss : 0.63 - acc : 0.658 time taken: 7.10\n",
      "epoch 18 - batch [111/112] - train loss: 0.36 - acc: 0.871 - valid loss : 0.63 - acc : 0.654 time taken: 7.02\n",
      "epoch 19 - batch [111/112] - train loss: 0.36 - acc: 0.870 - valid loss : 0.64 - acc : 0.655 time taken: 7.06\n",
      "CPU times: user 1min 58s, sys: 23.9 s, total: 2min 22s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%time training(ddcnn_rez1, 20, 2e-3, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Vocab(object):\n",
    "  \n",
    "  def __init__(self, L, doc_ids=None):    \n",
    "    if isinstance(L[0], list):\n",
    "      tokens = list(itertools.chain(*L))\n",
    "      self.token_counts = pd.Series(tokens).value_counts().to_frame().sort_index(ascending=True)\n",
    "      self.vocab = [\"<unk>\"] + self.token_counts.index.to_list()\n",
    "    else:\n",
    "      tokens = self.token_counts = pd.Series(L).value_counts().to_frame().sort_index(ascending=True)\n",
    "      self.vocab = self.token_counts.index.to_list()\n",
    "    self.vocab = list(set(self.vocab))\n",
    "    self.vocab = sorted(self.vocab)\n",
    "    if doc_ids is not None:\n",
    "      self.vocab = doc_ids + self.vocab \n",
    "    \n",
    "    self.w2i = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "    self.i2w = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "\n",
    "  def map_words2index(self, L):\n",
    "    return list(map(lambda x: self.w2i[x] if x in self.w2i else self.w2i['unk'], L))\n",
    "\n",
    "  def map_index2words(self, L):\n",
    "    return list(map(lambda x: self.i2w[x], L))\n",
    "\n",
    "  def map_dataset_words2index(self, L):\n",
    "    return np.array(list(map(self.map_words2index, L)))\n",
    "\n",
    "  def map_dataset_index2words(self, L):\n",
    "    return np.array(list(map(self.map_index2words, L)))\n",
    "\n",
    "  def get_counts(self):\n",
    "    return self.token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 71.1 ms, sys: 3.6 ms, total: 74.7 ms\n",
      "Wall time: 74.4 ms\n",
      "CPU times: user 57.9 ms, sys: 0 ns, total: 57.9 ms\n",
      "Wall time: 57.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time vocab = Vocab(text_train)\n",
    "%time train_x = vocab.map_dataset_words2index(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kirsten</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid-range</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid-section</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid-seventies</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid-to-low</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "kirsten        1\n",
       "mid-range      1\n",
       "mid-section    1\n",
       "mid-seventies  1\n",
       "mid-to-low     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = vocab.get_counts()\n",
    "counts.sort_values(0, inplace=True)\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_df.tsv',\n",
       " 'label_train.txt',\n",
       " 'raw',\n",
       " 'all_df_masks',\n",
       " 'label_test.txt',\n",
       " 'text_test.txt',\n",
       " 'text_all.txt',\n",
       " 'processed',\n",
       " 'test_df.tsv',\n",
       " 'text_train.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import to_undirected, is_undirected\n",
    "\n",
    "EMBED_DIM = 300\n",
    "\n",
    "class MRDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(MRDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['all_df_mask.tsv']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['mr_train.pt']\n",
    "    \n",
    "    def process(self):\n",
    "        df = pd.read_csv(self.raw_paths[0], sep=\"\\t\")\n",
    "#         df[\"text\"] = df.text.apply(clean_fn)\n",
    "        texts = list(map(lambda x: x.split(), df.text.values))\n",
    "        doc_ids = df.doc_id.values\n",
    "        labels = df.label.values\n",
    "        doc_label_dict = dict(zip(doc_ids, labels))\n",
    "        is_train_dict = dict(zip(doc_ids, df.train_mask.values))\n",
    "        vocab = Vocab(texts, doc_ids=df.doc_id.values.tolist())\n",
    "        text_int = vocab.map_dataset_words2index(texts)\n",
    "        \n",
    "        # nodes_idx mapping will be the same as vocab.i2w\n",
    "        n_nodes = len(vocab.w2i)\n",
    "        self.embed = nn.Embedding(n_nodes, EMBED_DIM)\n",
    "        nodes = self.embed(torch.tensor(range(n_nodes)))  # (vocab_size, EMBED_DIM)\n",
    "        \n",
    "        # edges\n",
    "        tfidf, cooccur_matrix, edge_feature2i, edge_i2feature, cooccur_idx = compute_TFIDF(df)\n",
    "        \n",
    "#         tfidf_edges, tfidf_attr = from_scipy_sparse_matrix(tfidf)\n",
    "#         cooccur_edges, cooccur_attr = from_scipy_sparse_matrix(cooccur_matrix)\n",
    "#         edge_index = torch.cat((tfidf_edges, cooccur_edges), axis=1)\n",
    "#         edge_attr = torch.cat((tfidf_attr, cooccur_attr))\n",
    "        \n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        # --- doc to words ---\n",
    "        for i, row in df.iterrows():\n",
    "            doc_id = vocab.w2i[row[\"doc_id\"]]\n",
    "            edges_ = zip([doc_id for _ in range(len(text_int[i]))], text_int[i])\n",
    "            edges_ = edges_ \n",
    "            edge_index.extend(list(edges_))\n",
    "        \n",
    "        edge_index = list(set(edge_index))  # a word may occur mult times in a doc\n",
    "        for (doc_id, word_id) in edge_index:\n",
    "            d, w = doc_id, edge_feature2i[vocab.i2w[word_id]]\n",
    "            edge_attr.append(tfidf[d, w])\n",
    "            \n",
    "        # --- words to doc ---\n",
    "        edge_index_back = [(e2, e1) for (e1, e2) in edge_index] \n",
    "        for (word_id, doc_id) in edge_index_back:\n",
    "            d, w = doc_id, edge_feature2i[vocab.i2w[word_id]]\n",
    "            edge_attr.append(tfidf[d, w])\n",
    "        edge_index += edge_index_back\n",
    "            \n",
    "        # --- word to word ---\n",
    "        for (w1, w2) in cooccur_idx: \n",
    "            # notice, w1, w2 are strs, convert to idx in respective spaces\n",
    "            w1_vocab_idx, w2_vocab_idx = vocab.w2i[w1], vocab.w2i[w2]\n",
    "            w1_edge_idx, w2_edge_idx = edge_feature2i[w1], edge_feature2i[w2]\n",
    "            weight = cooccur_matrix[w1_edge_idx, w2_edge_idx] if w1 != w2 else 1\n",
    "            edge_index.append((w1_vocab_idx, w2_vocab_idx))\n",
    "            edge_attr.append(weight)\n",
    "            \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "        # --- masks: ---\n",
    "        train_mask = []\n",
    "        test_mask = []\n",
    "        for i in range(n_nodes):\n",
    "            w = vocab.i2w[i]\n",
    "            if w in doc_label_dict:\n",
    "                is_train = is_train_dict[w]\n",
    "                train_mask.append(is_train)\n",
    "                test_mask.append(not is_train)\n",
    "            else:\n",
    "                train_mask.append(False)\n",
    "                test_mask.append(False)\n",
    "        labels = np.concatenate((labels, np.array([-1 for i in range(n_nodes-len(labels))])))\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        data_list = [Data(x=nodes, y=labels, edge_index=edge_index, edge_attr=edge_attr)]\n",
    "        data_list[0].train_mask = torch.tensor(train_mask)\n",
    "        data_list[0].test_mask = torch.tensor(test_mask)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix\n",
    "from copy import deepcopy\n",
    "import re, math\n",
    "\n",
    "def get_counts(df):\n",
    "    cooccur = defaultdict(int)\n",
    "    doc_freq = defaultdict(int)\n",
    "    for i, row in df.iterrows():\n",
    "        ts = row[\"text\"].split()\n",
    "        wpairs = set([(w1, w2) for w1 in ts for w2 in ts])\n",
    "        ws = set(ts)\n",
    "        for wpair in wpairs:\n",
    "            cooccur[wpair] += 1\n",
    "        for w in ws:\n",
    "            doc_freq[w] += 1\n",
    "    \n",
    "    return cooccur, doc_freq\n",
    "\n",
    "\n",
    "def compute_TFIDF(df):\n",
    "    corpus = df.text.values\n",
    "    counter = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "    tfidf_trans = TfidfTransformer()    \n",
    "    counts = counter.fit_transform(corpus)\n",
    "    \n",
    "    # -- tfidf \n",
    "    tfidf = tfidf_trans.fit_transform(counts)\n",
    "    # words have specific order when computing the features\n",
    "    # not neccessarily the same to the graph nodes -> needs conversion\n",
    "    features = counter.get_feature_names()\n",
    "    feature2i = dict(zip(features, range(len(features))))\n",
    "    i2feature = dict(zip(range(len(features)), features))\n",
    "    \n",
    "    # -- pmi -- \n",
    "    word_counts = counts.sum(axis=0)\n",
    "    n_words = word_counts.shape[1]\n",
    "    n_docs = tfidf.shape[0]\n",
    "    \n",
    "    cooccur, doc_freq = get_counts(df)\n",
    "    \n",
    "    idx = [k for k in cooccur]\n",
    "    rows, cols = tuple(zip(*idx))\n",
    "    \n",
    "    rows = [feature2i[w] for w in rows]\n",
    "    cols = [feature2i[w] for w in cols]\n",
    "    cooccur_data = [math.log(cooccur[(row, col)]*n_docs/(doc_freq[row]*doc_freq[col]))\n",
    "                    for (row, col) in idx]\n",
    "    cooccur_matrix = csr_matrix((cooccur_data, (rows, cols)), shape=(n_words, n_words))\n",
    "    \n",
    "    return tfidf, cooccur_matrix, feature2i, i2feature, idx\n",
    "\n",
    "def clean_fn(string):\n",
    "    string = re.sub(r\"\\r\", \" \", string)\n",
    "    string = re.sub(r\"\\n\", \" \", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9,!?'.\\-:]\", \" \", string)\n",
    "    string = re.sub(r'([A-Za-z0-9]+)\\s+([?!.]+)', r\"\\1\\2\", string)\n",
    "    string = re.sub(r'([A-Za-z0-9]+)[?!.]+', r'\\1.', string)\n",
    "    string = re.sub(r'(\\w+)n\\'t', r'\\1 not', string)\n",
    "    string = re.sub(r'(\\w+)\\'re', r'\\1 are', string)\n",
    "    string = re.sub(r'(\\w+)\\'ll', r'\\1 will', string)\n",
    "    string = re.sub(r\"\\s+\", \" \", string)\n",
    "    return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df[\"text\"] = all_df.text.apply(clean_fn)\n",
    "# %time tfidf, cooccur_matrix, feature2i, i2feature = compute_TFIDF(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.7 ms, sys: 20.1 ms, total: 76.8 ms\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%time dataset = MRDataset('data/mr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, dataset.num_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 49, loss = 0.4514, train_acc = 0.8177, valid_acc = 0.6092, time taken: 22.84\n",
      "epoch 99, loss = 0.2341, train_acc = 0.9454, valid_acc = 0.6362, time taken: 22.82\n",
      "epoch 149, loss = 0.1412, train_acc = 0.9766, valid_acc = 0.6415, time taken: 22.66\n",
      "epoch 199, loss = 0.0960, train_acc = 0.9855, valid_acc = 0.6474, time taken: 22.64\n",
      "epoch 249, loss = 0.0749, train_acc = 0.9900, valid_acc = 0.6322, time taken: 22.63\n",
      "epoch 299, loss = 0.0615, train_acc = 0.9938, valid_acc = 0.6528, time taken: 22.64\n",
      "epoch 349, loss = 0.0502, train_acc = 0.9927, valid_acc = 0.6536, time taken: 22.92\n",
      "epoch 399, loss = 0.0407, train_acc = 0.9951, valid_acc = 0.6522, time taken: 22.69\n",
      "epoch 449, loss = 0.0361, train_acc = 0.9959, valid_acc = 0.6415, time taken: 22.65\n",
      "epoch 499, loss = 0.0299, train_acc = 0.9972, valid_acc = 0.6500, time taken: 22.68\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(300).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005) #, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "start = time.time()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 49:\n",
    "        _, pred = model(data).max(dim=1)\n",
    "        train_correct = float(pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
    "        train_acc = train_correct / data.train_mask.sum().item()\n",
    "        valid_correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "        valid_acc = valid_correct / data.test_mask.sum().item()\n",
    "        \n",
    "        print(\"epoch {}, loss = {:.4f}, train_acc = {:.4f}, valid_acc = {:.4f}, time taken: {:.2f}\".format(epoch, loss, train_acc, valid_acc, time.time()-start))\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6612\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epoch 99, loss = 0.5432, train_acc = 0.7383, valid_acc = 0.5675, time taken: 4.04\\nepoch 199, loss = 0.4964, train_acc = 0.7577, valid_acc = 0.5521, time taken: 4.05\\nepoch 299, loss = 0.4750, train_acc = 0.7804, valid_acc = 0.5594, time taken: 4.20\\nepoch 399, loss = 0.4558, train_acc = 0.7860, valid_acc = 0.5627, time taken: 4.09\\nepoch 499, loss = 0.4447, train_acc = 0.7860, valid_acc = 0.5760, time taken: 3.99\\nepoch 599, loss = 0.4365, train_acc = 0.8029, valid_acc = 0.5619, time taken: 4.03\\nepoch 699, loss = 0.4366, train_acc = 0.8091, valid_acc = 0.5672, time taken: 4.17\\nepoch 799, loss = 0.4151, train_acc = 0.8143, valid_acc = 0.5701, time taken: 4.02\\nepoch 899, loss = 0.4292, train_acc = 0.8206, valid_acc = 0.5672, time taken: 3.96\\nepoch 999, loss = 0.4090, train_acc = 0.8213, valid_acc = 0.5726, time taken: 4.01\\nepoch 1099, loss = 0.4053, train_acc = 0.8271, valid_acc = 0.5777, time taken: 3.98\\nepoch 1199, loss = 0.3920, train_acc = 0.8275, valid_acc = 0.5664, time taken: 4.09\\nepoch 1299, loss = 0.3985, train_acc = 0.8236, valid_acc = 0.5726, time taken: 3.98\\nepoch 1399, loss = 0.3846, train_acc = 0.8308, valid_acc = 0.5799, time taken: 4.01\\nepoch 1499, loss = 0.3953, train_acc = 0.8334, valid_acc = 0.5678, time taken: 3.99\\nepoch 1599, loss = 0.3906, train_acc = 0.8399, valid_acc = 0.5746, time taken: 3.97\\nepoch 1699, loss = 0.3839, train_acc = 0.8388, valid_acc = 0.5732, time taken: 4.04\\nepoch 1799, loss = 0.3695, train_acc = 0.8399, valid_acc = 0.5518, time taken: 4.08\\nepoch 1899, loss = 0.3750, train_acc = 0.8344, valid_acc = 0.5644, time taken: 4.16\\nepoch 1999, loss = 0.3806, train_acc = 0.8412, valid_acc = 0.5822, time taken: 4.18\\n\\nAccuracy: 0.5968'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Only Words to Edge\n",
    "\"\"\"epoch 99, loss = 0.5432, train_acc = 0.7383, valid_acc = 0.5675, time taken: 4.04\n",
    "epoch 199, loss = 0.4964, train_acc = 0.7577, valid_acc = 0.5521, time taken: 4.05\n",
    "epoch 299, loss = 0.4750, train_acc = 0.7804, valid_acc = 0.5594, time taken: 4.20\n",
    "epoch 399, loss = 0.4558, train_acc = 0.7860, valid_acc = 0.5627, time taken: 4.09\n",
    "epoch 499, loss = 0.4447, train_acc = 0.7860, valid_acc = 0.5760, time taken: 3.99\n",
    "epoch 599, loss = 0.4365, train_acc = 0.8029, valid_acc = 0.5619, time taken: 4.03\n",
    "epoch 699, loss = 0.4366, train_acc = 0.8091, valid_acc = 0.5672, time taken: 4.17\n",
    "epoch 799, loss = 0.4151, train_acc = 0.8143, valid_acc = 0.5701, time taken: 4.02\n",
    "epoch 899, loss = 0.4292, train_acc = 0.8206, valid_acc = 0.5672, time taken: 3.96\n",
    "epoch 999, loss = 0.4090, train_acc = 0.8213, valid_acc = 0.5726, time taken: 4.01\n",
    "epoch 1099, loss = 0.4053, train_acc = 0.8271, valid_acc = 0.5777, time taken: 3.98\n",
    "epoch 1199, loss = 0.3920, train_acc = 0.8275, valid_acc = 0.5664, time taken: 4.09\n",
    "epoch 1299, loss = 0.3985, train_acc = 0.8236, valid_acc = 0.5726, time taken: 3.98\n",
    "epoch 1399, loss = 0.3846, train_acc = 0.8308, valid_acc = 0.5799, time taken: 4.01\n",
    "epoch 1499, loss = 0.3953, train_acc = 0.8334, valid_acc = 0.5678, time taken: 3.99\n",
    "epoch 1599, loss = 0.3906, train_acc = 0.8399, valid_acc = 0.5746, time taken: 3.97\n",
    "epoch 1699, loss = 0.3839, train_acc = 0.8388, valid_acc = 0.5732, time taken: 4.04\n",
    "epoch 1799, loss = 0.3695, train_acc = 0.8399, valid_acc = 0.5518, time taken: 4.08\n",
    "epoch 1899, loss = 0.3750, train_acc = 0.8344, valid_acc = 0.5644, time taken: 4.16\n",
    "epoch 1999, loss = 0.3806, train_acc = 0.8412, valid_acc = 0.5822, time taken: 4.18\n",
    "\n",
    "Accuracy: 0.5968\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
